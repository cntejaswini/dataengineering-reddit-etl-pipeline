# Reddit Data Engineering Pipeline ğŸš€

A full-stack data pipeline to extract, transform, and load Reddit data into Amazon Redshift using Apache Airflow, Celery, PostgreSQL, S3, AWS Glue, and Athena.

---

## ğŸ“Œ Table of Contents

* [Project Summary](#project-summary)
* [Architecture](#architecture)
* [Prerequisites](#prerequisites)
* [Environment Setup](#environment-setup)
* [Running the Pipeline](#running-the-pipeline)
* [Demo](#demo)

---

## ğŸ“„ Project Summary

This end-to-end data pipeline is built to automate the ETL (Extract, Transform, Load) process for Reddit data. The flow involves pulling data from Redditâ€™s API, staging it in S3, transforming it using AWS Glue and Athena, and loading the final dataset into Amazon Redshift for analytics.

---

## ğŸ— Architecture

The pipeline includes the following components:

* **Reddit API** â€“ Acts as the primary data source.
* **Apache Airflow + Celery** â€“ Handles orchestration and distributed task execution.
* **PostgreSQL** â€“ Stores intermediate data and Airflow metadata.
* **Amazon S3** â€“ Stores raw JSON files pulled from Reddit.
* **AWS Glue** â€“ Manages the schema and performs transformation tasks.
* **Amazon Athena** â€“ Runs SQL-based transformations directly on S3.
* **Amazon Redshift** â€“ Final destination for analytics-ready structured data.

Architecture diagram available in `RedditDataEngineering.png`.

---

## âœ… Prerequisites

Ensure you have the following:

* An AWS account with access to S3, Glue, Athena, and Redshift
* Reddit API credentials
* Python 3.9+ installed
* Docker and Docker Compose installed

---

## âš™ï¸ Environment Setup

Clone the repository:

```bash
git clone https://github.com/airscholar/RedditDataEngineering.git
cd RedditDataEngineering
```

Create and activate a virtual environment:

```bash
python3 -m venv venv
source venv/bin/activate
```

Install Python dependencies:

```bash
pip install -r requirements.txt
```

Configure credentials:

```bash
mv config/config.conf.example config/config.conf
# Edit config.conf with your Reddit and AWS credentials
```

---

## ğŸš€ Running the Pipeline

Start the services using Docker Compose:

```bash
docker-compose up -d
```

Access the Airflow UI:

```bash
http://localhost:8080
```

Use Airflow to trigger and monitor the ETL DAG that processes Reddit data from extraction to loading in Redshift.

---

## ğŸ¥ Demo

*(Include video walkthrough link or instructions for local testing/demo if applicable)*

---

Let me know if youâ€™d like a `README.md` file generated from this directly or want to push it to your GitHub.
